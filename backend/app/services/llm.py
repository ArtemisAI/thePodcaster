"""Abstraction layer around the Ollama REST API."""

import httpx
import json
import logging
from typing import Dict, List, Any # Ensure List and Any are imported

from ..config import settings

# Get a logger for this module
logger = logging.getLogger(__name__)

PROMPT_TEMPLATES = {
    "title_summary": """
Based on the following podcast transcript, please suggest 3 concise and engaging titles and a short 2-3 sentence summary.

Transcript:
{transcript}

Suggestions (provide in JSON format with keys "titles": ["title1", "title2", "title3"] and "summary": "your summary"):
""",
    "title_only": """
Based on the following podcast transcript, please suggest 3 concise and engaging titles.

Transcript:
{transcript}

Suggestions (provide in JSON format with key "titles": ["title1", "title2", "title3"]):
""",
    "summary_only": """
Based on the following podcast transcript, please provide a short 2-3 sentence summary.

Transcript:
{transcript}

Suggestions (provide in JSON format with key "summary": "your summary"):
"""
}

async def generate_suggestions(transcript: str, prompt_type: str = "title_summary") -> Dict[str, Any]:
    """
    Generates suggestions (titles, summary) from a transcript using Ollama.

    Args:
        transcript: The podcast transcript text.
        prompt_type: The type of prompt to use ("title_summary", "title_only", "summary_only").
                     Defaults to "title_summary".

    Returns:
        A dictionary containing suggestions, e.g., {"titles": [...], "summary": "..."}.
        Returns a dictionary with an "error" key on failure.

    Raises:
        ValueError: If an invalid prompt_type is provided.
        httpx.HTTPStatusError: If Ollama returns an HTTP error status.
        httpx.RequestError: For other request-related issues (e.g., connection error).
        Exception: For other unexpected errors during the process.
    """
    if prompt_type not in PROMPT_TEMPLATES:
        logger.error(f"Invalid prompt_type specified: {prompt_type}")
        raise ValueError(f"Invalid prompt_type: {prompt_type}. Available types: {list(PROMPT_TEMPLATES.keys())}")

    prompt = PROMPT_TEMPLATES[prompt_type].format(transcript=transcript)
    
    ollama_url = settings.OLLAMA_URL
    model_name = settings.OLLAMA_DEFAULT_MODEL
    
    request_payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": False, # Expect a single JSON response
        "format": "json"  # Request Ollama to output JSON directly
    }

    logger.info(f"Sending request to Ollama. URL: {ollama_url}/api/generate, Model: {model_name}, Prompt Type: {prompt_type}")
    logger.debug(f"Ollama request payload (prompt truncated): {{'model': '{model_name}', 'prompt': '{prompt[:100]}...', 'stream': False, 'format': 'json'}}")

    # Increased timeout for potentially long LLM responses
    async with httpx.AsyncClient(timeout=120.0) as client: 
        try:
            response = await client.post(f"{ollama_url}/api/generate", json=request_payload)
            # This will raise an HTTPStatusError if the response status is 4xx or 5xx
            response.raise_for_status() 

            response_data = response.json()
            logger.debug(f"Raw response data from Ollama: {response_data}")
            
            # Ollama with format="json" and stream=False returns a single JSON object.
            # The 'response' field of this object contains the actual JSON string generated by the model.
            if 'response' in response_data and isinstance(response_data['response'], str):
                try:
                    suggestions_json = json.loads(response_data['response'])
                    logger.info(f"Successfully received and parsed suggestions from Ollama for prompt type '{prompt_type}'.")
                    
                    # Basic validation of the parsed JSON structure based on prompt_type
                    if prompt_type == "title_summary" and not ("titles" in suggestions_json and isinstance(suggestions_json.get("titles"), list) and "summary" in suggestions_json and isinstance(suggestions_json.get("summary"), str)):
                        logger.warning(f"LLM output for '{prompt_type}' did not contain expected 'titles' (list) and 'summary' (str). Received: {suggestions_json}")
                        # Return what we have, or a more specific error structure
                        return {"titles": suggestions_json.get("titles", []), "summary": suggestions_json.get("summary", "LLM output structure mismatch.")}
                    elif prompt_type == "title_only" and not ("titles" in suggestions_json and isinstance(suggestions_json.get("titles"), list)):
                        logger.warning(f"LLM output for '{prompt_type}' did not contain expected 'titles' (list). Received: {suggestions_json}")
                        return {"titles": suggestions_json.get("titles", [])}
                    elif prompt_type == "summary_only" and not ("summary" in suggestions_json and isinstance(suggestions_json.get("summary"), str)):
                        logger.warning(f"LLM output for '{prompt_type}' did not contain expected 'summary' (str). Received: {suggestions_json}")
                        return {"summary": suggestions_json.get("summary", "LLM output structure mismatch.")}

                    return suggestions_json
                
                except json.JSONDecodeError as e:
                    logger.error(f"Failed to parse JSON from Ollama's 'response' field. Content: '{response_data['response']}'. Error: {e}", exc_info=True)
                    # Return a structured error that can be handled by the API layer
                    return {"error": "Failed to parse LLM response JSON", "details": response_data.get('response')}
            else:
                logger.error(f"Ollama response did not contain a 'response' field or it wasn't a string. Full response: {response_data}")
                return {"error": "Ollama response missing or malformed 'response' field", "details": str(response_data)}

        except httpx.HTTPStatusError as e:
            # Log the detailed error response from Ollama if available
            error_body = e.response.text if e.response else "No response body."
            logger.error(f"HTTP error {e.response.status_code} from Ollama: {error_body}", exc_info=True)
            raise # Re-raise to be handled by the API route or Celery task
        except httpx.RequestError as e:
            logger.error(f"Request error occurred while calling Ollama (URL: {e.request.url}): {e}", exc_info=True)
            raise 
        except Exception as e:
            # Catch any other unexpected errors
            logger.error(f"An unexpected error occurred in generate_suggestions: {e}", exc_info=True)
            raise

# Example usage (can be run with `python -m backend.app.services.llm` if __main__ block is added)
# async def main():
#     logging.basicConfig(level=logging.DEBUG) # Setup basic logging for the example
#     sample_transcript = "Your very long podcast transcript here..."
#     try:
#         results = await generate_suggestions(sample_transcript, "title_summary")
#         print("Suggestions:", results)
#     except Exception as e:
#         print("Error:", e)

# if __name__ == "__main__":
#     import asyncio
#     asyncio.run(main())
