"""Endpoints that interact with the local LLM via Ollama."""

# Planned endpoints:
# * POST /ai/suggest â€“ Provide transcript; returns title & summary.
#   Body: {"transcript_id": int, "tone": "clickbaity|formal|casual"}

# TODO: call services.llm.generate_title_summary
